nc.sd.sp <- st_read("https://pauldelamater.net/geog215/exams/NCLEA.geojson")
## Convert layer from WGS 84 coordinate system to NC State Plane
nc.sd.sp <- st_transform(nc.sd.sp, crs = 2264)
## Perform join of csv and geojson layer by the following attributes
nc.sd.sp.join <- merge(nc.sd.sp, dat.nc, by.x = "GEOID", by.y = "LEA_ID", all.x = TRUE)
## Create map
tm_shape(nc.sd.sp) +
tm_polygons("UTDR17",
style = "jenks",
palette = "YlOrRd")
knitr::opts_chunk$set(echo = TRUE)
## Create map
tm_shape(nc.sd.sp) +
tm_polygons("AVESCORE",
style = "jenks",
palette = "YlOrRd")
nc.all <- which(dat$STATE == "NC" & dat$GROUP == "all")
#Stored in an object/variable all observations that were in North Carolina and had a group attribute of "all"
dat.nc <- dat[nc.all,]
#Stored in a variable and created a new file named "dat.nc" with the values from nc.all
dat.nc <- na.omit(dat.nc)
#Removes missing values from the new table
dat.nc$SCOREDEV <- dat.nc$AVESCORE - 5.5
#Calculate the average score for each observation minus 5.5 in a new variable in a new column
n.nc.leas <- nrow(dat.nc)
#Calculate rows in dat.nc
nc.lea.ave <- mean(dat.nc$AVESCORE)
#Calculated mean of the variable AVESCORE for each observation.
n.above.ave <- length(which(dat.nc$AVESCORE >= 5.5))
#Calculate length of observations whose average score is greater or equal to 5.5
n.above.ave.p <- 100 * n.above.ave / n.nc.leas
#Calculate percentage of the mean of AVESCORE devided by n.above.ave
## Create map
tm_shape(nc.sd.sp) +
tm_polygons("AVESCORE",
style = "jenks",
palette = "YlOrRd")
knitr::opts_chunk$set(echo = TRUE)
## Create map
tm_shape(nc.sd.sp) +
tm_polygons("dat.nc$AVESCORE",
style = "jenks",
palette = "YlOrRd")
knitr::opts_chunk$set(echo = TRUE)
## Create map
tm_shape(nc.sd.sp) +
tm_polygons(dat.nc$AVESCORE,
style = "jenks",
palette = "YlOrRd")
knitr::opts_chunk$set(echo = TRUE)
## Create map
tm_shape(nc.sd.sp) +
tm_polygons(nc.lea.ave,
style = "jenks",
palette = "YlOrRd")
knitr::opts_chunk$set(echo = TRUE)
## Create map
tm_shape(nc.sd.sp) +
tm_polygons("nc.lea.ave"",
style = "jenks",
knitr::opts_chunk$set(echo = TRUE)
## Create map
tm_shape(nc.sd.sp) +
tm_polygons("nc.lea.ave",
style = "jenks",
palette = "YlOrRd")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
download.file("https://delamater.web.unc.edu/files/2019/09/SEDA_SD_SCORES.csv", "C:/Users/dasorto/Desktop/GEOG215/Assignments/exam1/educachieve.csv")
dat <- read_csv("educachieve.csv")
ncol(dat)
nrow(dat)
nc.all <- which(dat$STATE == "NC" & dat$GROUP == "all")
#Stored in an object/variable all observations that were in North Carolina and had a group attribute of "all"
dat.nc <- dat[nc.all,]
#Stored in a variable and created a new file named "dat.nc" with the values from nc.all
dat.nc <- na.omit(dat.nc)
#Removes missing values from the new table
dat.nc$SCOREDEV <- dat.nc$AVESCORE - 5.5
#Calculate the average score for each observation minus 5.5 in a new variable in a new column
n.nc.leas <- nrow(dat.nc)
#Calculate rows in dat.nc
nc.lea.ave <- mean(dat.nc$AVESCORE)
#Calculated mean of the variable AVESCORE for each observation.
n.above.ave <- length(which(dat.nc$AVESCORE >= 5.5))
#Calculate length of observations whose average score is greater or equal to 5.5
n.above.ave.p <- 100 * n.above.ave / n.nc.leas
#Calculate percentage of the mean of AVESCORE devided by n.above.ave
## Load spatial packages/libraries
library(sf)
library(tmap)
## Read a file (geojson) from the internet
nc.sd.sp <- st_read("https://pauldelamater.net/geog215/exams/NCLEA.geojson")
## Convert layer from WGS 84 coordinate system to NC State Plane
nc.sd.sp <- st_transform(nc.sd.sp, crs = 2264)
## Perform join of csv and geojson layer by the following attributes
nc.sd.sp.join <- merge(nc.sd.sp, dat.nc, by.x = "GEOID", by.y = "LEA_ID", all.x = TRUE)
## Create map
tm_shape(nc.sd.sp) +
tm_polygons("nc.lea.ave",
style = "jenks",
palette = "YlOrRd")
knitr::opts_chunk$set(echo = TRUE)
## Create map
tm_shape(nc.sd.sp) +
tm_polygons("nc.lea.ave",
style = "jenks",
palette = "PuOr")
knitr::opts_chunk$set(echo = TRUE)
## Create map
tm_shape(nc.sd.sp) +
tm_polygons("AREA",
style = "jenks",
palette = "PuOr")
knitr::opts_chunk$set(echo = TRUE)
## Create map
tm_shape(nc.sd.sp.join) +
tm_polygons("AREA",
style = "jenks",
palette = "PuOr")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
download.file("https://delamater.web.unc.edu/files/2019/09/SEDA_SD_SCORES.csv", "C:/Users/dasorto/Desktop/GEOG215/Assignments/exam1/educachieve.csv")
dat <- read_csv("educachieve.csv")
ncol(dat)
nrow(dat)
nc.all <- which(dat$STATE == "NC" & dat$GROUP == "all")
#Stored in an object/variable all observations that were in North Carolina and had a group attribute of "all"
dat.nc <- dat[nc.all,]
#Stored in a variable and created a new file named "dat.nc" with the values from nc.all
dat.nc <- na.omit(dat.nc)
#Removes missing values from the new table
dat.nc$SCOREDEV <- dat.nc$AVESCORE - 5.5
#Calculate the average score for each observation minus 5.5 in a new variable in a new column
n.nc.leas <- nrow(dat.nc)
#Calculate rows in dat.nc
nc.lea.ave <- mean(dat.nc$AVESCORE)
#Calculated mean of the variable AVESCORE for each observation.
n.above.ave <- length(which(dat.nc$AVESCORE >= 5.5))
#Calculate length of observations whose average score is greater or equal to 5.5
n.above.ave.p <- 100 * n.above.ave / n.nc.leas
#Calculate percentage of the mean of AVESCORE devided by n.above.ave
## Load spatial packages/libraries
library(sf)
library(tmap)
## Read a file (geojson) from the internet
nc.sd.sp <- st_read("https://pauldelamater.net/geog215/exams/NCLEA.geojson")
## Convert layer from WGS 84 coordinate system to NC State Plane
nc.sd.sp <- st_transform(nc.sd.sp, crs = 2264)
## Perform join of csv and geojson layer by the following attributes
nc.sd.sp.join <- merge(nc.sd.sp, dat.nc, by.x = "GEOID", by.y = "LEA_ID", all.x = TRUE)
## Create map
tm_shape(nc.sd.sp) +
tm_polygons("AREA",
style = "jenks",
palette = "PuOr")
knitr::opts_chunk$set(echo = TRUE)
## Create map
##tm_shape(nc.sd.sp) +
##tm_polygons("AREA",
##style = "jenks",
##palette = "PuOr")
####
#### R script to perform a table join and create a map
#### for GEOG 215, Assignment #6
####
#### Shows how to retrieve geojson data as well!
## Load required libraries
library(tidyverse)
library(sf)
library(geojsonsf)
library(tmap)
#### Read geojson data directly as sf object
#### Big-ish file... might take a minute to retrieve
nc_bg_polys <- geojson_sf("https://opendata.arcgis.com/datasets/7d3c48062ffc4cf781835bfa530497e4_8.geojson")
####
#### R script to perform a table join and create a map
#### for GEOG 215, Assignment #6
####
#### Shows how to retrieve geojson data as well!
## Load required libraries
library(tidyverse)
library(sf)
library(geojsonsf)
library(tmap)
#### Read geojson data directly as sf object
#### Big-ish file... might take a minute to retrieve
nc_bg_polys <- geojson_sf("https://opendata.arcgis.com/datasets/7d3c48062ffc4cf781835bfa530497e4_8.geojson")
nc_bg_polys <- geojson_sf("https://opendata.arcgis.com/datasets/7d3c48062ffc4cf781835bfa530497e4_8.geojson")
library(tidyverse)
library(sf)
library(geojsonsf)
library(tmap)
#### Read geojson data directly as sf object
#### Big-ish file... might take a minute to retrieve
nc_bg_polys <- geojson_sf("https://opendata.arcgis.com/datasets/7d3c48062ffc4cf781835bfa530497e4_8.geojson")
library(tmap)
library(rgdal)
me <- readOGR("./Data", "Income")
qtm(me, fill="Income")
library(tmap)
library(rgdal)
me <- readOGR("./Data", "Income")
qtm(me, fill="Income", fill.style="quantile",
fill.n=4,
fill.palette="Greens",
legend.text.size = 0.5,
layout.legend.position = c("right", "bottom"))
library(tmap)
library(rgdal)
me <- readOGR("./Data", "Income")
q()
library(rgdal)
library(tmap)
cavac <- read.csv("T:/Data/Lab8/gas.csv")
me <- readOGR("./Data/STATES.shp", "Shapefile")
library(rgdal)
me <- readOGR("T:/Data/Lab8/STATES.shp", "Shapefile")
me <- readOGR("T:/Data/Lab8/STATES.shp", "States")
me <- readOGR("T:/Data/Lab8/STATES.shp", "STATES")
View(cavac)
View(me)
me@data <- left_join(me@data, dat, by=c("State" = "STATE_NAME"))
library(dplyr)
me@data <- left_join(me@data, dat, by=c("State" = "STATE_NAME"))
me@data <- left_join(me@data, cavac, by=c("State" = "STATE_NAME"))
View(cavac)
me@data <- left_join(me@data, cavac, by=c("State." = "STATE_NAME"))
me@data <- left_join(me@data, cavac, by=c("STATE_NAME" = "State."))
qtm(me, fill="me@data", fill.style="pretty", title="Regular Price Map", legend.outside=TRUE)
qtm(me, fill="me", fill.style="pretty", title="Regular Price Map", legend.outside=TRUE)
qtm(me, fill="Regular.", fill.style="pretty", title="Regular Price Map", legend.outside=TRUE)
qtm(me, fill="Regular.", fill.style="pretty", title="Regular Price Map", legend.outside=TRUE, n=5)
qtm(me, fill="Regular.", fill.style="pretty", title="Regular Price Map", legend.outside=TRUE)
d100 <- buffer(Aug_UTM, width=50000, quadsegs=10)
d200 <- buffer(Aug_UTM, width=100000, quadsegs=10)
d300 <- buffer(Aug_UTM, width=150000, quadsegs=10)
d100 <- buffer(Aug_UTM, width=200000, quadsegs=10)
d300 <- buffer(Aug_UTM, width=300000, quadsegs=10)
# Let's check the circles on a Maine outline
tm_shape(d300) +tm_borders() +tm_shape(d200) +tm_borders() + tm_shape(d100) +tm_borders() +tm_shape(me) +tm_borders()
d50 <- buffer(Aug_UTM, width=50000, quadsegs=10)
d100 <- buffer(Aug_UTM, width=100000, quadsegs=10)
d150 <- buffer(Aug_UTM, width=150000, quadsegs=10)
d200 <- buffer(Aug_UTM, width=200000, quadsegs=10)
d300 <- buffer(Aug_UTM, width=300000, quadsegs=10)
# Let's check the circles on a Maine outline
tm_shape(d300) +tm_borders() +tm_shape(d200) +tm_borders() + tm_shape(d100) +tm_borders() +tm_shape(me) +tm_borders()
d50 <- buffer(Aug_UTM, width=50000, quadsegs=10)
library(raster)
d50 <- buffer(Aug_UTM, width=50000, quadsegs=10)
library(raster)
library(rgdal)
library(tmap)
library(dplyr)
d50 <- buffer(Aug_UTM, width=50000, quadsegs=10)
library(raster)
library(rgdal)
library(tmap)
library(dplyr)
d50 <- buffer(Aug_UTM, width=50000, quadsegs=10)
library(rgeos)
d50 <- buffer(Aug_UTM, width=50000, quadsegs=10)
d100 <- buffer(Aug_UTM, width=100000, quadsegs=10)
d150 <- buffer(Aug_UTM, width=150000, quadsegs=10)
d200 <- buffer(Aug_UTM, width=200000, quadsegs=10)
d300 <- buffer(Aug_UTM, width=300000, quadsegs=10)
# Let's check the circles on a Maine outline
tm_shape(d300) +tm_borders() +tm_shape(d200) +tm_borders() + tm_shape(d100) +tm_borders() +tm_shape(me) +tm_borders()
Aug_UTM <- spTransform(Aug_ll, CRS("+init=epsg:26919"))
d50 <- buffer(Aug_UTM, width=50000, quadsegs=10)
Aug_UTM <- spTransform(Aug_ll, CRS("+init=epsg:26919"))
Aug_UTM <- spTransform(Aug_ll, CRS("+init=epsg:26919"))
library(raster)
library(rgdal)
library(tmap)
library(dplyr)
library(rgeos)
install.packages("devtools")
devtools::install_github("benmarwick/wordcountaddin", type = "source", dependencies = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(sf)
library(tmap)
library(readxl)
library(kableExtra)
## Read in County spatial data layer (polygon)
cal.cty <- st_read("./Data/orangecounty_tr1.shp", quiet = TRUE)
tm_shape(cal.cty) +
tm_polygons() +
tm_shape(points) +
tm_dots(col = "red")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(sf)
library(tmap)
library(readxl)
library(kableExtra)
accidentsdataset <- read_excel("./Data/OrangeCountyAccidents.xlsx")
points <- st_as_sf(accidentsdataset, coords = c("Start_Lat", "Start_Lng"))
tm_shape(points) + tm_dots ()
# Create summary information holder
summary.table <- tibble(Measure = c("Observations",
"Minimum",
"Maximum",
"Mean",
"Median",
"Standard Deviation"),
Cases = c(sum(!is.na(accidentsdataset$`Severity`)),
min(accidentsdataset$`Severity`, na.rm = TRUE),
max(accidentsdataset$`Severity`, na.rm = TRUE),
mean(accidentsdataset$`Severity`, na.rm = TRUE),
median(accidentsdataset$`Severity`, na.rm = TRUE),
sd(accidentsdataset$`Severity`, na.rm = TRUE)
)
)
## Print nice version of the table
kable(summary.table,
digits = 1,
format.args = list(big.mark = ",",
scientific = FALSE,
drop0trailing = TRUE),
caption = "Severity
Shows the severity of the accident, a number between 1 and 4, where 1 indicates the least impact on traffic (i.e., short delay ") %>%
kable_styling(bootstrap_options = c("striped",
"hover",
"condensed",
"responsive"),
full_width = F)
## plot histogram
ggplot(accidentsdataset,
aes(x = `Severity`)) +
geom_histogram(binwidth = 0.5) +
xlab("Severity
Shows the severity of the accident, a number between 1 and 4, where 1 indicates the least impact on traffic (i.e., short delay).") +
ggtitle("Histogram of the Severity of each Car Traffic Accident in Orange County") +
theme_minimal()
## Read in County spatial data layer (polygon)
cal.cty <- st_read("./Data/orangecounty_tr1.shp", quiet = TRUE)
tm_shape(cal.cty) +
tm_polygons() +
tm_shape(points) +
tm_dots(col = "red")
#cal.dtp.sj.cty <- st_join(cal.cty, points)
wordcountaddin:::text_stats()
orangecounty <- st_read("./Data/dissolvedorangecounty.shp", quiet = TRUE)
setwd("C:/Users/dasorto/Desktop/GEOG215/Sorto_Geog215_FinalProject")
orangecounty <- st_read("./Data/dissolvedorangecounty.shp", quiet = TRUE)
orangecounty <- st_read("./Data/dissolvedorangecounty.shp", quiet = TRUE)
orangecounty <- st_read("/Data/dissolvedorangecounty.shp", quiet = TRUE)
orangecounty <- st_read("./Data/dissolved_orangecounty.shp", quiet = TRUE)
accidentsdataset.ppp <- as(accidentsdataset, "Spatial") %>% as("ppp")
points.ppp <- as(points, "Spatial") %>% as("ppp")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(sf)
library(tmap)
library(readxl)
library(kableExtra)
library(sp)
library(spdep)
library(spatstat)
library(maptools)
points.ppp <- as(points, "Spatial") %>% as("ppp")
## Note that these are UNMARKED points
marks(points.ppp) <- NULL
## Provide it with the Durham STUDY AREA bounds as a window
Window(points.ppp) <- as(orangecounty, "Spatial") %>% as("owin")
orangecounty.projected <- st_transform(orangecounty,
crs = 32119)
points.projected <- st_transform(points.ppp,
crs = 32119)
orangecounty <- st_read("./Data/dissolved_orangecounty.shp", quiet = TRUE)
orangecounty.projected <- st_transform(orangecounty,
crs = 32119)
points.projected <- st_transform(points,
crs = 32119)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(sf)
library(tmap)
library(readxl)
library(kableExtra)
library(sp)
library(spdep)
library(spatstat)
library(maptools)
accidentsdataset <- read_excel("./Data/OrangeCountyAccidents.xlsx")
points <- st_as_sf(accidentsdataset, coords = c("Start_Lat", "Start_Lng"))
tm_shape(points) + tm_dots ()
orangecounty <- st_read("./Data/dissolved_orangecounty.shp", quiet = TRUE)
orangecounty.projected <- st_transform(orangecounty,
crs = 32119)
points.projected <- st_transform(points,
crs = 32119)
st_crs(points)
points.projected <- st_set_crs(points,
crs = 32119)
st_crs(points.projected)
points.projected <- st_set_crs(points,
32119)
points.ppp <- as(points.projected, "Spatial") %>% as("ppp")
## Note that these are UNMARKED points
marks(points.ppp) <- NULL
## Provide it with the Durham STUDY AREA bounds as a window
Window(points.ppp) <- as(orangecounty, "Spatial") %>% as("owin")
## Provide it with the Durham STUDY AREA bounds as a window
Window(points.ppp) <- as(orangecounty.projected, "Spatial") %>% as("owin")
## Plot the layer to ensure that the boundary is properly defined
plot(points.ppp,
main = NULL,
cols = rgb(0, 0, 0, 0.5),
pch = 20)
points.ppp.quadrat <- quadratcount(points.ppp,
nx = 5,
ny = 10)
points.ppp.quadrat <- quadratcount(points.ppp,nx = 5, ny = 10)
points.ppp.quadrat <- quadratcount(points.ppp, nx = 10, ny = 10)
## Plot the layer with quadrat counts
plot(points.ppp,
main = NULL,
cols = "grey70",
pch = 20)
plot(points.ppp.quadrat,
add = TRUE,
cex = 0.75)
points.ppp.quadrat.test <- quadrat.test(points.ppp,
nx = 10,
ny = 10)
## Plot the layer with quadrat counts
plot(points.ppp,
main = NULL,
cols = "grey70",
pch = 20)
plot(points.ppp.quadrat,
add = TRUE,
cex = 0.75)
points.ppp.quadrat.test <- quadrat.test(points.ppp,
nx = 7,
ny = 7)
tm_shape(points.projected) + tm_dots()
devtools::install_github("benmarwick/wordcountaddin", type = "source", dependencies = TRUE)
orangecounty <- st_read("./Data/dissolved_orangecounty.shp", quiet = TRUE)
orangecounty.projected <- st_transform(orangecounty,
crs = 32119)
points.projected <- st_set_crs(points,
32119)
points.ppp <- as(points.projected, "Spatial") %>% as("ppp")
## Note that these are UNMARKED points
marks(points.ppp) <- NULL
## Provide it with the Durham STUDY AREA bounds as a window
Window(points.ppp) <- as(orangecounty.projected, "Spatial") %>% as("owin")
## Plot the layer to ensure that the boundary is properly defined
plot(points.ppp,
main = NULL,
cols = rgb(0, 0, 0, 0.5),
pch = 20)
points.ppp.quadrat <- quadratcount(points.ppp, nx = 10, ny = 10)
## Plot the layer with quadrat counts
plot(points.ppp,
main = NULL,
cols = "grey70",
pch = 20)
plot(points.ppp.quadrat,
add = TRUE,
cex = 0.75)
points.ppp.quadrat.test <- quadrat.test(points.ppp,
nx = 7,
ny = 7)
points.ppp.quadrat.test <- quadrat.test(points.ppp,
nx = 5,
ny = 5)
tm_shape(points.projected) + tm_dots()
tm_shape(points.projected) + tm_dots()
## Calculate and print Variance to Mean Ratio
points.ppp.VMR <- var(points.ppp.quadrat.test$observed) / mean(points.ppp.quadrat.test$observed)
points.ppp.VMR
## Print test results to screen
points.ppp.quadrat.test
## Remember the warning about expected counts?
## Let's increase the size of that quadrats to increase
## the number of Fire Stations in each
## We will use twice as many rows as columns, given data shape
points.ppp.quadrat <- quadratcount(points.ppp,
nx = 2,
ny = 4)
points.ppp.quadrat <- quadratcount(points.ppp,
nx = 4,
ny = 4)
## Plot the quadrat counts
plot(points.ppp.quadrat,
cex = 0.75)
points.ppp.quadrat.test <- quadrat.test(points.ppp,
nx = 4,
ny = 4)
## Calculate and print Variance to Mean Ratio
points.ppp.VMR <- var(points.ppp.quadrat.test$observed) / mean(points.ppp.quadrat.test$observed)
points.ppp.VMR
## Print test results to screen
points.ppp.quadrat.test
tm_shape(orangecounty) +tm_polygons()
tm_shape(points.projected) + tm_dots()
tm_shape(orangecounty) +tm_polygons()
tm_shape(orangecounty) +
tm_polygons() +
tm_shape(points.projected) +
tm_dots(col = "red")
tm_shape(points.projected) + tm_dots()
tm_shape(orangecounty) +tm_polygons()
wordcountaddin:::text_stats()
devtools::install_github("benmarwick/wordcountaddin", type = "source", dependencies = TRUE)
wordcountaddin:::text_stats()
